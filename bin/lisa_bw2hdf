#!/usr/bin/env python
""" input a bigwig, preprocess them to lisa
regulatory potential and 1kb read count, generate hdf5 file
"""
import fire
from lisa.data import EpigenomeData
import h5py
import numpy as np
import os

class HDF(object):
    """ interface for processing single bigwig to hdf5 """
    def __init__(self, species, epigenome, prefix):
        """ `epigenome` can be epigenome type, e.g. H3K27ac or ATAC-seq or DNase
            `epigenome` can also be covariates, e.g., GC or mappability

            prefix is used to label output HDF5 files, for epigenome sample, use `project name`
                                                       for covariates, use `covarates`
        """
        self.species = species
        self.epigenome = epigenome
        self.prefix = prefix

    def get_regpotential_hdf(self, bigwig):
        """ input one bigwig file, generate temporary
        hdf5 file for RP and read count """
        data = EpigenomeData(self.species, self.epigenome)
        data.create_RP_h5(bigwig, self.prefix)

    def merge_reg_potential_hdf(self, *hdf5):
        """ processing a list of reg potential hdf5 files into one merged hdf5,
        input should be from the same epigenome type, e.g. H3K4me3,
        or from a list of covariates, e.g. GC.
        """
        with h5py.File(hdf5[0]) as inf:
            nrp = inf["RP"].shape[0]
            refseq = inf["RefSeq"][...]
        with h5py.File('%s.%s.reg.h5' % (self.prefix, self.epigenome), "a") as store:
            refseq_arr = store.create_dataset("RefSeq",
                                              shape=(len(refseq), ),
                                              dtype='S200',
                                              compression='gzip',
                                              shuffle=True, fletcher32=True)
            refseq_arr[...] = refseq

            RP = store.create_dataset("RP", dtype=np.float32, shape=(nrp, len(hdf5)), compression='gzip', shuffle=True, fletcher32=True)
            ids = store.create_dataset("IDs",
                                       shape=(len(hdf5), ), dtype='S50',
                                       compression='gzip', shuffle=True, fletcher32=True)

            iids = []
            for i, d in enumerate(hdf5):
                with h5py.File(d) as inf:
                    RP[:,i] = inf["RP"][:,0]
                store.flush()
                iids.append(str.encode(self.prefix + ".%s" % os.path.basename(d).split('.')[0], 'utf-8'))

            ids[...] = np.array(iids)
            store.flush()

    def get_readcount_hdf(self, bigwig):
        """ input one bigwig file, generate temporary
        hdf5 file for RP and read count """
        data = EpigenomeData(self.species, self.epigenome)
        data.create_Count_h5(bigwig, self.prefix)

    def merge_readcount_hdf(self, *hdf5):
        """ merge multiple hdf5 generated from process_one_bigwig """
        with h5py.File(hdf5[0]) as inf:
            nc = inf["OrderCount"].shape[0]

        with h5py.File('%s.%s.readcount.h5' % (self.prefix, self.epigenome), "a") as store:
            ct = store.create_dataset("OrderCount", dtype=np.float32, shape=(nc, len(hdf5)), compression='gzip', shuffle=True, fletcher32=True)
            ids = store.create_dataset("IDs", shape=(len(hdf5), ),
                                       dtype='S50',
                                       compression='gzip', shuffle=True, fletcher32=True)

            iids = []
            for i, d in enumerate(hdf5):
                with h5py.File(d) as inf:
                    ct[:,i] = inf["OrderCount"][:,0]
                store.flush()
                iids.append(str.encode(self.prefix + ".%s" % os.path.basename(d).split('.')[0], 'utf-8'))
            ids[...] = np.array(iids)
            store.flush()


if __name__ == '__main__':
    fire.Fire(HDF)
